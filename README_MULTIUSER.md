# Ollama Multi-User Benchmark Tool

A Python application to simulate 2. **Configure concurrent users**: Select number of users (1-5000)
3. **System warnings**: High user counts (>500) show resource warningsultiple concurrent users and benchmark Ollama models' user handling potential, saving detailed results to an Excel file with multiple sheets.

## Features

- **Concurrent User Simulation**: Simulates 1-5000 concurrent users making requests simultaneously
- **User Handling Potential**: Measures how many users a model can effectively handle
- **Comprehensive Metrics**: Tracks success rates, response times, throughput, and resource usage
- **Multi-Sheet Excel Export**: Saves summary and detailed user performance data
- **System Resource Monitoring**: Monitors VRAM, CPU, and memory usage during tests
- **Realistic User Behavior**: Varies prompts, temperatures, and request timing
- **High-Load Optimization**: Efficiently handles thousands of concurrent users

## Multi-User Metrics Collected

### Summary Sheet
- **LLM Model**: Name of the model being tested
- **Quantization**: Detected quantization level
- **Concurrent Users**: Number of simulated users
- **Active Users**: Users who successfully made at least one request
- **Active User Rate**: Percentage of users who were able to participate
- **Total/Successful/Failed Requests**: Request statistics
- **Success Rate**: Percentage of successful requests
- **User Handling Potential**: Successful requests per second per user
- **Aggregate Throughput**: Total tokens per second across all users
- **Average Response Time**: Mean response time across all users
- **System Resource Usage**: Max VRAM, CPU, and memory usage
- **GPU Information**: Graphics card, driver, and CUDA versions

### User Details Sheet
- **Individual User Performance**: Per-user request statistics
- **Response Time Metrics**: Min, max, and average response times per user
- **Token Generation**: Tokens generated by each user
- **Error Tracking**: Timeouts and errors per user

## Prerequisites

1. **Ollama installed and running**: Make sure Ollama is installed and running on your system
2. **Python 3.7+**: Required for the application
3. **NVIDIA GPU** (optional): For GPU monitoring and VRAM tracking

## Installation

1. Install required Python packages:
```bash
pip install requests pandas psutil openpyxl
```

2. Make sure Ollama is running:
```bash
ollama serve
```

3. Ensure you have some models installed:
```bash
ollama pull llama2
ollama pull mistral
# etc.
```

## Usage

1. **Run the multi-user benchmark**:
```bash
python3 ollama_multiuser_benchmark.py
```

2. **Select a model**: Choose from available models
3. **Configure concurrent users**: Select number of users (1-20)
4. **Confirm and start**: The benchmark runs for 2 minutes
5. **View results**: Results displayed on screen and saved to Excel

## Understanding the Results

### User Handling Potential
This is the key metric for multi-user performance:
- **Formula**: Successful requests per second per user
- **Higher values**: Better concurrent user handling
- **Interpretation**: 
  - 0.5+ req/s/user = Excellent multi-user performance
  - 0.3-0.5 req/s/user = Good performance
  - 0.1-0.3 req/s/user = Moderate performance
  - <0.1 req/s/user = Poor multi-user performance

### Success Rate
- **90%+**: Excellent stability under load
- **80-90%**: Good performance with minor issues
- **70-80%**: Moderate performance, may struggle with high load
- **<70%**: Poor performance under concurrent load

### Response Time Patterns
- **Consistent times**: Good load balancing
- **High variance**: Performance degradation under load
- **Increasing timeouts**: System overload

## Example Output

```
Ollama Multi-User Benchmark Tool
========================================
✓ Connected to Ollama
✓ System info collected - GPU: NVIDIA GeForce RTX 4090

Available Ollama Models:
--------------------------------------------------
1. llama2:7b (3.83 GB)
2. mistral:7b (4.11 GB)

Select a model (1-2): 1

Concurrent User Configuration:
Available range: 1-5000

Recommended starting points:
  • Light load: 1-10 users
  • Medium load: 10-50 users
  • Heavy load: 50-200 users
  • Stress test: 200-1000 users
  • Extreme test: 1000+ users

⚠️  WARNING: Testing with 2500 concurrent users!
This may:
  • Consume significant CPU and memory
  • Create many network connections
  • Potentially impact system stability
Are you sure you want to proceed? (y/n): y

Configuration Summary:
Model: llama2:7b
Concurrent Users: 10
Duration: 120 seconds

Start multi-user benchmark? (y/n): y

Starting multi-user benchmark for model: llama2:7b
Concurrent users: 10
Duration: 120 seconds
============================================================
Starting concurrent user simulation...
Progress: ██████████████████████████████ 120.0s (Remaining: 0.0s)
Multi-user benchmark completed!

================================================================================
MULTI-USER BENCHMARK RESULTS
================================================================================
Model: llama2:7b
Quantization: Q4
GPU: NVIDIA GeForce RTX 4090
Driver: 535.129.03
CUDA: 12.2
--------------------------------------------------------------------------------
CONCURRENT USER PERFORMANCE:
Concurrent Users: 10
Total Requests: 324
Successful Requests: 318
Failed Requests: 6
Success Rate: 98.15%
Timeouts: 2
Errors: 4
--------------------------------------------------------------------------------
PERFORMANCE METRICS:
User Handling Potential: 0.265 req/s/user
Aggregate Throughput: 142.30 tokens/s
Average Response Time: 3.24 seconds
Total Tokens Generated: 17076
Test Duration: 120.05 seconds
--------------------------------------------------------------------------------
SYSTEM RESOURCE USAGE:
Max VRAM Usage: 6234.50 MB
Max CPU Usage: 45.2%
Max Memory Usage: 23.1%
================================================================================

Results saved to: ollama_multiuser_benchmark_results.xlsx
  - Summary sheet: Aggregate metrics
  - User_Details sheet: Individual user performance
```

## Comparison with Single-User Tool

| Feature | Single-User Tool | Multi-User Tool |
|---------|------------------|-----------------|
| **Focus** | Raw performance | Concurrent handling |
| **Users** | 1 | 1-20 concurrent |
| **Metrics** | Throughput, latency | User handling potential |
| **Use Case** | Maximum performance | Real-world load testing |
| **Output** | Single sheet Excel | Multi-sheet Excel |
| **Behavior** | Consistent prompts | Varied user simulation |

## Use Cases

### Model Selection for Production
- Test how models perform under realistic concurrent load
- Compare user handling capacity between different models
- Identify optimal concurrent user limits

### Infrastructure Planning
- Determine server requirements for expected user loads
- Plan scaling strategies based on user handling metrics
- Optimize resource allocation

### Performance Optimization
- Test impact of quantization on multi-user performance
- Evaluate GPU memory vs. concurrent user trade-offs
- Benchmark different model sizes for concurrent workloads

## Troubleshooting

### High Failure Rates
- Reduce concurrent users
- Increase timeout values in code
- Check system resources

### Poor User Handling Potential
- Try smaller/faster models
- Reduce concurrent users
- Optimize system configuration

### System Resource Issues
- Monitor GPU memory usage
- Ensure adequate CPU/RAM
- Consider model quantization

## Files Generated

- `ollama_multiuser_benchmark_results.xlsx`: Main results file
  - **Summary** sheet: Aggregate performance metrics
  - **User_Details** sheet: Individual user statistics
- Fallback CSV files if Excel writing fails

The tool provides comprehensive insights into how well Ollama models handle concurrent users, helping you make informed decisions for production deployments and resource planning.
